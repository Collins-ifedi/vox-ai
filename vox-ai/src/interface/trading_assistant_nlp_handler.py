# trading_assistant_nlp_handler.py

import sys
import os
import logging
import asyncio
import time
import json
import re
import random
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
import yaml
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np
import requests # Use requests for synchronous calls

# --- START OF MODIFIED SECTION ---
# --- Database Imports ---
import databases
import sqlalchemy

# --- Database Setup ---
# Load DATABASE_URL from environment variables, similar to Server.py
DATABASE_URL = os.getenv("DATABASE_URL")
if not DATABASE_URL:
    logging.warning("DATABASE_URL environment variable not set. Database features will be disabled.")
    database = None
    metadata = None
else:
    database = databases.Database(DATABASE_URL)
    metadata = sqlalchemy.MetaData()

# Define the conversation_history table to match Server.py
# This is necessary for the handler to interact with the database directly.
# A foreign key to 'users' is defined, so the 'users' table schema is also needed for context.
users = sqlalchemy.Table(
    "users",
    metadata,
    sqlalchemy.Column("userId", sqlalchemy.String, primary_key=True),
    # Add other columns to match the server's definition if needed for full context,
    # but for history, only the key is essential.
    sqlalchemy.Column("email", sqlalchemy.String, unique=True, index=True),
)

conversation_history_table = sqlalchemy.Table(
    "conversation_history",
    metadata,
    sqlalchemy.Column("id", sqlalchemy.Integer, primary_key=True, autoincrement=True),
    sqlalchemy.Column("user_id", sqlalchemy.String, sqlalchemy.ForeignKey("users.userId"), index=True),
    sqlalchemy.Column("role", sqlalchemy.String),  # 'user' or 'assistant'
    sqlalchemy.Column("content", sqlalchemy.Text),
    sqlalchemy.Column("timestamp", sqlalchemy.DateTime, default=datetime.utcnow),
)
# --- END OF MODIFIED SECTION ---


# --- PATH SETUP ---
# Ensure this path is correct for your environment
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


try:
# --- IMPORTS ---
# Assuming these imports are correct relative to PROJECT_ROOT
    from src.data.market_data import MarketDataFetcher
    from src.analysis.technical_analysis import TechnicalAnalyzer
    from src.analysis.pattern_analyzer import analyze_patterns
    # from src.analysis.onchain_analysis import run_onchain_analysis # Unintegrated
    from src.analysis.sentiment_analysis import get_sentiment_snapshot
    from src.analysis.macro_economy_indicator import analyze_macro_indicators, MACRO_INDICATORS # Ensure MACRO_INDICATORS is imported
    # UPDATED IMPORT: Points to the new signal generator
    from src.signal_generation.signal_generator import SignalGenerator
    from src.analysis.combined_strategies import CombinedStrategiesRunner
    from src.interface.image import ImageIO, get_openai_api_key
    from src.interface.visual_analyzer import VisualAnalyzer # INTEGRATED: New visual analyzer
    from utils.coin_symbol_mapper import get_symbol, generate_symbol_variants
    from utils.logger import get_logger
    logger = get_logger("MarketData")
except (ImportError, ModuleNotFoundError) as e:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("MarketData_Fallback")
    logger.error(f"Failed to import from 'utils': {e}. Using fallback functions.")
    def get_symbol(identifier: str) -> Optional[str]: return identifier.upper().split('/')[0]
    def generate_symbol_variants(identifier: str, quotes: List[str]) -> Dict:
        return {"slash_separated": [f"{identifier}/{q}" for q in quotes], "concatenated": [f"{identifier}{q}" for q in quotes]}
    # Using updated path based on common project structures if anomaly_detection is a module
    try:
        from src.anomaly_detection.anomaly_detection import AnomalyDetector # Prefer module if exists
    except ImportError:
        from src.anomaly_detection.anomaly_detector import AnomalyDetector # Fallback to file
except ImportError as e:
    # Provide more specific error feedback
    logging.error(f"Failed to import necessary modules: {e}. Check PYTHONPATH, project structure at '{PROJECT_ROOT}', and ensure all dependencies (e.g., 'anomaly_detection') are correctly installed and named.", exc_info=True)
    # Define dummy classes/functions if critical modules are missing to allow script to load (for basic interaction)
    # This is a fallback and indicates a setup issue.
    class MarketDataFetcher: pass
    class TechnicalAnalyzer: pass
    def analyze_patterns(df): return {}
    async def get_sentiment_snapshot(symbol): return {}
    def analyze_macro_indicators(indicators, coin_name): return []
    class SignalGenerator: pass # Dummy class
    class CombinedStrategiesRunner: pass
    class ImageIO: pass
    class VisualAnalyzer: pass # Dummy class for fallback
    MACRO_INDICATORS = []
    def get_logger(name): return logging.getLogger(name) # Basic logger
    def get_symbol(name): return None
    class AnomalyDetector: pass


# --- CONFIGURATION ---
# Initialize logger early, handling potential import issues if get_logger was part of the failing block
if 'get_logger' not in globals() or not callable(get_logger):
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logger = logging.getLogger("TradingAssistantNLPHandler_FallbackLogger")
    logger.warning("Custom logger 'get_logger' failed to import. Using fallback basic logger.")
else:
    logger = get_logger("TradingAssistantNLPHandler")

if not logger.handlers: # Check if handlers were already set by get_logger or if it's the fallback
    log_file_path = os.path.join(PROJECT_ROOT, 'logs', 'trading_assistant_nlp.log')
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)
    # Reconfigure basicConfig if it's the fallback or get_logger didn't set handlers
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(log_file_path)
        ]
    )
    logger.info("Logger (re)initialized with INFO level and file handler in NLP Handler.")

# --- CONSTANTS ---
CONFIG_PATH = os.path.join(PROJECT_ROOT, "config", "credentials.yaml")
# REMOVED: HISTORY_FILE is now user-specific and managed by the database.
os.makedirs(os.path.join(PROJECT_ROOT, "data"), exist_ok=True)


# Hardcoded Intents (as requested)
INTENTS = [
    "predict_price", "generate_signal", "explain_signal", "fetch_market_data", "analyze_indicators",
    "analyze_patterns", "analyze_sentiment", "analyze_macro", "analyze_visual_document", # INTEGRATED: Added visual analysis intent
    "world_economy", "crypto_general", "personal_conversation", "casual_chat",
    "analyze_anomalies", "fetch_macro_news", "tell_joke", "start_conversation",
    "compliment_received", "farewell", "unknown"
]
DEFAULT_SYMBOL = "BTC" # This is a coin name, not a trading pair for this default
DEFAULT_EXCHANGE = "bitget"
DEFAULT_TIMEFRAME = "1h" # Changed to 1h to align with new SignalGenerator's default
DEFAULT_LIMIT = 600
DEFAULT_LLM_MODEL = "meta-llama/llama-4-maverick" # Using a hypothetical updated model name
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
MAX_HISTORY_LEN = 50 # Limit saved history size

# --- INTENT KEYWORDS ---
INTENT_KEYWORDS = {
    "predict_price": ["predict", "prediction", "forecast", "price prediction", "future price", "what will the price do"],
    "generate_signal": ["signal", "signals", "trade", "buy", "sell", "long", "short", "position"],
    "explain_signal": ["explain", "why", "reason", "signal explanation", "tell me more"],
    "fetch_market_data": ["market data", "price", "ohlc", "candle", "chart data", "ticker"],
    "analyze_indicators": ["indicators", "technical", "technical analysis", "rsi", "macd", "bollinger", "ema", "ta"],
    "analyze_patterns": ["patterns", "pattern analysis", "chart pattern", "candlestick", "bullish engulfing", "candlestick pattern" "bearish harami", "analyze pattern", "pattern"],
    "analyze_sentiment": ["sentiment", "fear greed", "social sentiment", "market mood", "fg index"],
    "analyze_macro": ["macro", "economy", "gdp", "inflation", "interest rate", "cpi", "economic indicator"],
    # INTEGRATED: Keywords for the new visual analysis intent
    "analyze_visual_document": ["analyze file", "analyse file", "read this image", "describe this picture", "read this pdf", "analyze this document", "look at this image", "what does this document say"],
    "world_economy": ["world economy", "global market", "economic impact", "recession", "fiscal policy", "geopolitics"],
    "crypto_general": ["crypto", "cryptocurrency", "blockchain", "bitcoin", "ethereum", "defi", "nft", "future of crypto", "about btc", "digital crypto assets", "crypto asset", "decentralized finance", "web3"],
    "personal_conversation": ["how are you", "tell me about you", "your day", "feeling", "opinion", "any problem", "any issues"],
    "casual_chat": ["hi", "hello", "what's up", "hey", "how's it going", "good morning", "good afternoon", "thank you", "thanks"],
    "analyze_anomalies": ["anomaly", "anomalies", "unusual activity", "irregular", "abnormal", "strange behavior", "detect anomaly", "pump and dump", "manipulation", "market control", "manipulate", "controlling the market"],
    "fetch_macro_news": ["macro news", "news today", "economic news", "geopolitical news"],
    "tell_joke": ["tell me a joke", "crack a joke", "funny", "say something funny"],
    "start_conversation": ["start a conversation", "let's talk", "discuss something"],
    "compliment_received": ["good job", "well done", "nice work", "great job", "awesome bot", "you're helpful"],
    "farewell": ["exit", "quit", "bye", "goodbye", "see you", "later"]
}

# --- Dataclasses for Config ---
@dataclass
class OpenRouterConfig:
    """Configuration specific to OpenRouter API interaction."""
    api_key: Optional[str] = None
    model: str = DEFAULT_LLM_MODEL
    max_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9

@dataclass
class IntentConfig:
    """Configuration for intent detection."""
    intents: List[str] = field(default_factory=lambda: INTENTS)
    use_llm_for_intent: bool = False # Default to keyword-based for speed, LLM as fallback
    max_history_for_intent: int = 3
    prompt_template: str = (
        "You are a helpful classification assistant. Given the conversation history (if any) and the latest user query, "
        "classify the user's primary intent based ONLY on the following list:\n{intents}\n\n"
        "Conversation History (most recent first):\n{history}\n\n"
        "Latest User Query: '{query}'\n\n"
        "Intent:"
    )

@dataclass
class OutputConfig:
    """Configuration for output formatting and typing effect."""
    typing_delay: float = 0.02
    max_response_length: int = 2000 # Max length for LLM-generated parts of responses
    verbose: bool = True # Controls typing effect, could be tied to global log level too
    initial_welcome_message: str = (
        "Hey there! I'm your crypto trading assistant from Onitsha, Nigeria! "
        "I can help with signals, analysis (technical, patterns, sentiment, macro, anomalies, visual documents), "
        "discuss crypto trends, the world economy, or just chat. Kedu ka ị melu? How can I help?"
    )

@dataclass
class ModuleConfig:
    """Configuration for module settings (defaults for analysis parameters)."""
    default_symbol: str = DEFAULT_SYMBOL # e.g., "BTC" for general context if not specified
    default_exchange: str = DEFAULT_EXCHANGE
    default_timeframe: str = DEFAULT_TIMEFRAME
    default_limit: int = DEFAULT_LIMIT


class TradingAssistantNLPHandler:
    """
    Handles NLP queries, interacts with analysis modules, uses a configured LLM via requests
    for dynamic conversation and classification, and produces human-readable outputs.
    """

    def __init__(self, user_id: int, config_path: str = CONFIG_PATH):
        """
        Initializes the handler for a specific user.

        Args:
            user_id (int): The unique identifier for the user.
            config_path (str): The path to the configuration file.
        """
        self.user_id = user_id
        # self.config_path is kept for loading non-db configs
        self.config_path = config_path
        self.intent_config = IntentConfig()
        self.output_config = OutputConfig()
        self.module_config = ModuleConfig()
        self.llm_config = OpenRouterConfig()

        self._load_config() # Load configurations from YAML and environment

        # --- INTEGRATED: Initialize VisualAnalyzer ---
        self.visual_analyzer: Optional[VisualAnalyzer] = None
        try:
            # VisualAnalyzer will find the OPENAI_API_KEY from the environment itself.
            # We just need to handle the case where it's not found.
            self.visual_analyzer = VisualAnalyzer()
            logger.info("✅ VisualAnalyzer initialized successfully.")
        except ValueError as e:
            # This happens if the API key is not found.
            logger.warning(f"VisualAnalyzer not initialized and will be disabled. Reason: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during VisualAnalyzer initialization: {e}", exc_info=True)
        # --- END INTEGRATION ---

        self.llm_available = bool(self.llm_config.api_key)
        if self.llm_available:
            logger.info(f"OpenRouter client configured for model: {self.llm_config.model}")
        else:
            logger.error("OpenRouter API key not found. LLM features will be unavailable. Check OPENROUTER_API_KEY env var or credentials.yaml.")
        
        # --- START OF MODIFIED SECTION ---
        # History is now initialized as empty and loaded from DB in the async initialize() method.
        self.conversation_history: List[Dict[str, Any]] = []
        # --- END OF MODIFIED SECTION ---
        self.last_signal: Optional[Dict[str, Any]] = None

        self.known_symbols: set = set() # Will be populated by the async initialize() method

        # Initialize module instances as None; they will be created on demand.
        self.signal_generator: Optional[SignalGenerator] = None
        self.market_fetcher: Optional[MarketDataFetcher] = None
        self.technical_analyzer: Optional[TechnicalAnalyzer] = None
        self.anomaly_detector: Optional[AnomalyDetector] = None

        # Context for conversation flow
        self.context: Dict[str, Any] = {"last_symbol_extracted": None, "last_topic": None, "last_intent": None}

        logger.info(f"TradingAssistantNLPHandler initialized for User ID: {self.user_id}.")
        if not self.llm_available:
            logger.warning(f"LLM client not available for User ID {self.user_id}. LLM-based features will be limited. Keyword-based functions will still work.")

    # --- REMOVED: _history_file_path property is no longer needed. ---

    async def initialize(self):
        """
        Asynchronously initializes the handler by loading necessary data,
        such as caching symbols and loading conversation history from the database.
        """
        # --- START OF MODIFIED SECTION ---
        if database and not database.is_connected:
            try:
                await database.connect()
                logger.info("Database connection established for NLP Handler.")
            except Exception as e:
                logger.error(f"Failed to connect to the database in NLP Handler: {e}", exc_info=True)
        # --- END OF MODIFIED SECTION ---

        # These can run concurrently
        await asyncio.gather(
            asyncio.to_thread(self._load_and_cache_symbols_sync),
            self._load_history_from_db()
        )

    def _load_and_cache_symbols_sync(self):
        """
        Synchronously fetches the list of all available trading symbols
        using the standard `ccxt` library, which uses `requests`.
        This list is used to VERIFY symbols extracted by the LLM.
        """
        logger.info(f"Synchronously fetching and caching symbols from '{self.module_config.default_exchange}' for verification...")
        try:
            import ccxt
            exchange_class = getattr(ccxt, self.module_config.default_exchange)
            exchange = exchange_class()

            markets = exchange.load_markets()
            base_symbols = {market['base'] for market in markets.values() if 'base' in market}

            if not base_symbols:
                raise ValueError("Market data was loaded but yielded an empty list of symbols.")

            self.known_symbols = base_symbols
            logger.info(f"Successfully cached {len(self.known_symbols)} unique symbols from '{self.module_config.default_exchange}'.")

        except (ccxt.NetworkError, ccxt.ExchangeError, ValueError) as e:
            logger.warning(f"Could not dynamically fetch symbols due to an exchange/network error: {e}. Falling back to a basic list.")
            self.known_symbols = {"BTC", "ETH", "SOL", "XRP", "ADA", "DOGE", "LTC", "BNB", "MATIC", "LINK", "XLM"}
        except ImportError:
            logger.error("The 'ccxt' library is not installed. Cannot dynamically fetch symbols. Please run 'pip install ccxt'.")
            self.known_symbols = {"BTC", "ETH", "SOL", "XRP", "ADA", "DOGE", "LTC", "BNB", "MATIC", "LINK", "XLM"}
        except Exception as e:
            logger.error(f"An unexpected error occurred during symbol fetching: {e}", exc_info=True)
            logger.warning("Falling back to a basic hardcoded list of symbols due to an unexpected error.")
            self.known_symbols = {"BTC", "ETH", "SOL", "XRP", "ADA", "DOGE", "LTC", "BNB", "MATIC", "LINK", "XLM"}


    def _load_config(self):
        # Try loading OpenRouter API key from environment variable first
        openrouter_key_env = os.environ.get("OPENROUTER_API_KEY")
        if openrouter_key_env:
            logger.info("Loaded OpenRouter API key from OPENROUTER_API_KEY environment variable.")
            self.llm_config.api_key = openrouter_key_env

        try:
            config_path_obj = Path(self.config_path)
            if config_path_obj.exists():
                with config_path_obj.open("r") as f:
                    config_yaml = yaml.safe_load(f) or {}

                if not self.llm_config.api_key:
                    openrouter_creds = config_yaml.get("openrouter", {})
                    self.llm_config.api_key = openrouter_creds.get("api_key")
                    if self.llm_config.api_key:
                        logger.info("Loaded OpenRouter API key from credentials.yaml.")

                llm_params = config_yaml.get("llm_params", {})
                self.llm_config.model = llm_params.get("model", self.llm_config.model)
                self.llm_config.max_tokens = llm_params.get("max_tokens", self.llm_config.max_tokens)
                self.llm_config.temperature = llm_params.get("temperature", self.llm_config.temperature)
                self.llm_config.top_p = llm_params.get("top_p", self.llm_config.top_p)

                nlp_config = config_yaml.get("nlp_handler", {})
                self.output_config.typing_delay = nlp_config.get("typing_delay", self.output_config.typing_delay)
                self.output_config.verbose = nlp_config.get("verbose", self.output_config.verbose)
                self.intent_config.max_history_for_intent = nlp_config.get("max_history_for_intent", self.intent_config.max_history_for_intent)
                self.intent_config.use_llm_for_intent = nlp_config.get("use_llm_for_intent", self.intent_config.use_llm_for_intent)

                loaded_welcome = nlp_config.get("initial_welcome_message")
                if loaded_welcome and isinstance(loaded_welcome, str):
                    phrases_to_remove = ["on-chain, ", ", on-chain", "on-chain analysis, ", ", on-chain analysis"]
                    for phrase in phrases_to_remove:
                        loaded_welcome = loaded_welcome.replace(phrase, "")
                    self.output_config.initial_welcome_message = loaded_welcome.replace("  ", " ")


                module_defaults = config_yaml.get("module_defaults", {})
                self.module_config.default_symbol = module_defaults.get("default_symbol", self.module_config.default_symbol)
                self.module_config.default_exchange = module_defaults.get("default_exchange", self.module_config.default_exchange)
                self.module_config.default_timeframe = module_defaults.get("default_timeframe", self.module_config.default_timeframe)
                self.module_config.default_limit = module_defaults.get("default_limit", self.module_config.default_limit)

                logger.info("Successfully loaded configurations from credentials.yaml.")
            else:
                logger.warning(f"Configuration file not found at {self.config_path}. Using default settings and environment variables if available.")
                if not self.llm_config.api_key:
                    logger.error("OpenRouter API key is not set via environment variables or config file. LLM functionalities will be disabled.")

        except yaml.YAMLError as e:
            logger.error(f"Error parsing YAML config file {self.config_path}: {e}. Using default settings.", exc_info=True)
        except Exception as e:
            logger.error(f"An unexpected error occurred while loading config file {self.config_path}: {e}. Using default settings.", exc_info=True)

    # --- START OF MODIFIED SECTION ---
    async def _load_history_from_db(self) -> List[Dict[str, Any]]:
        """Loads conversation history from the database for the current user."""
        if not database or not database.is_connected:
            logger.warning(f"Cannot load history for user {self.user_id}: Database is not connected.")
            return []

        try:
            query = (
                conversation_history_table.select()
                .where(conversation_history_table.c.user_id == self.user_id)
                .order_by(conversation_history_table.c.timestamp.desc())
                .limit(MAX_HISTORY_LEN)
            )
            results = await database.fetch_all(query)
            
            # Results are newest-first, so we reverse to get chronological order for processing.
            history = [
                {"role": row["role"], "content": row["content"]} for row in reversed(results)
            ]
            
            logger.info(f"Successfully loaded {len(history)} messages from DB for user {self.user_id}.")
            self.conversation_history = history
            return history
        except Exception as e:
            logger.error(f"Failed to load conversation history from database for user {self.user_id}: {e}", exc_info=True)
            return []

    async def _add_message_to_db(self, role: str, content: str):
        """Saves a single message to the database."""
        if not database or not database.is_connected:
            logger.warning(f"Cannot save message for user {self.user_id}: Database is not connected.")
            return

        try:
            query = conversation_history_table.insert().values(
                user_id=self.user_id,
                role=role,
                content=content,
                timestamp=datetime.utcnow()
            )
            await database.execute(query)
        except Exception as e:
            logger.error(f"Failed to save message to database for user {self.user_id}: {e}", exc_info=True)

    # --- REMOVED: _save_history method is no longer needed. ---
    # --- END OF MODIFIED SECTION ---

    def prediction_price(self, trading_symbol: str) -> Optional[Dict[str, Any]]:
        """
        Integrates multiple analysis modules to predict future price trends.
        Generates a visualization of the prediction.

        Args:
            trading_symbol (str): The trading symbol to analyze (e.g., 'BTC/USDT').

        Returns:
            Optional[Dict[str, Any]]: A dictionary with prediction details, or None on failure.
        """
        logger.info(f"Initiating price prediction pipeline for {trading_symbol} for user {self.user_id}...")
        reasoning_parts = []
        final_score = 0.0

        # --- 1. Fetch Market Data ---
        try:
            if not self.market_fetcher:
                self.market_fetcher = MarketDataFetcher(timeframe=self.module_config.default_timeframe, limit=self.module_config.default_limit)
            
            # Using a larger limit for better analysis
            df_ohlcv = self.market_fetcher.fetch_ohlcv(identifier=trading_symbol, limit=400)

            if df_ohlcv is None or df_ohlcv.empty:
                logger.error(f"Prediction failed: Could not fetch OHLCV data for {trading_symbol}.")
                return None
            reasoning_parts.append(f"Fetched {len(df_ohlcv)} recent candles for {trading_symbol} on the {self.module_config.default_timeframe} timeframe.")
        except Exception as e:
            logger.error(f"Prediction failed at data fetching stage for {trading_symbol}: {e}", exc_info=True)
            return None

        # --- 2. Technical Analysis ---
        try:
            tech_analyzer = TechnicalAnalyzer(df_ohlcv)
            df_with_indicators = tech_analyzer.generate_all_indicators()
            tech_summary = tech_analyzer.get_structured_summary()
            if tech_summary and tech_summary.get('sentiment'):
                ta_score = tech_summary['sentiment'].get('numeric_score', 0.0)
                final_score += ta_score * 0.4 # Weight TA score heavily
                reasoning_parts.append(f"Technical Analysis indicates a {tech_summary['sentiment']['category']} sentiment (Score: {ta_score:.2f}). {tech_summary['sentiment']['narrative']}")
            else:
                 reasoning_parts.append("Technical analysis summary could not be generated.")
        except Exception as e:
            logger.warning(f"Technical analysis step failed for {trading_symbol}: {e}", exc_info=True)
            reasoning_parts.append("An error occurred during technical analysis.")

        # --- 3. Candlestick Pattern Analysis ---
        try:
            df_ohlcv.attrs['symbol'] = trading_symbol.split('/')[0]
            pattern_results = analyze_patterns(df_ohlcv.copy())
            if pattern_results:
                pattern_sentiment = pattern_results.get('sentiment', 'Neutral')
                bull_patterns = pattern_results.get('bullish_signals', 0)
                bear_patterns = pattern_results.get('bearish_signals', 0)
                latest_patterns = pattern_results.get('latest_patterns', [])
                
                pattern_score = 0.0
                if pattern_sentiment == "Bullish":
                    pattern_score = 0.25
                elif pattern_sentiment == "Bearish":
                    pattern_score = -0.25

                final_score += pattern_score
                reasoning = f"Candlestick Pattern Analysis shows a {pattern_sentiment} bias with {bull_patterns} bullish and {bear_patterns} bearish signals recently."
                if latest_patterns:
                    reasoning += f" The latest candle formed the following pattern(s): {', '.join(latest_patterns)}."
                reasoning_parts.append(reasoning)
            else:
                 reasoning_parts.append("Candlestick pattern analysis yielded no results.")
        except Exception as e:
            logger.warning(f"Pattern analysis step failed for {trading_symbol}: {e}", exc_info=True)
            reasoning_parts.append("An error occurred during pattern analysis.")

        # --- 4. Combined Strategies Analysis ---
        try:
            strategy_runner = CombinedStrategiesRunner(timeframe=self.module_config.default_timeframe, limit=400)
            strategy_results = strategy_runner.run_all_strategies(symbol=trading_symbol, df_ohlcv=df_with_indicators.copy())

            if strategy_results:
                long_signals = 0
                short_signals = 0
                strategy_reasons = []
                for name, result in strategy_results.items():
                    if result.get('signal') == 'long':
                        long_signals += 1
                        strategy_reasons.append(name)
                    elif result.get('signal') == 'short':
                        short_signals += 1
                        strategy_reasons.append(name)
                
                strategy_score = (long_signals - short_signals) * 0.1 # Each strategy has a small influence
                final_score += strategy_score
                reasoning = f"Automated Strategy Analysis found {long_signals} 'long' signals and {short_signals} 'short' signals."
                if strategy_reasons:
                    reasoning += f" Active strategies include: {', '.join(strategy_reasons)}."
                reasoning_parts.append(reasoning)
            else:
                reasoning_parts.append("Combined strategy analysis yielded no results.")
        except Exception as e:
            logger.warning(f"Combined strategies step failed for {trading_symbol}: {e}", exc_info=True)
            reasoning_parts.append("An error occurred during strategy analysis.")

        # --- 5. Final Prediction Logic ---
        final_score = np.clip(final_score, -1.0, 1.0)
        
        if final_score > 0.35:
            prediction = "Bullish"
            color_theme = "glowing green, vibrant emerald"
            trend_desc = "strong upward momentum"
        elif final_score < -0.35:
            prediction = "Bearish"
            color_theme = "ominous red, deep crimson"
            trend_desc = "significant downward pressure"
        else:
            prediction = "Neutral"
            color_theme = "calm yellow, stable grey"
            trend_desc = "a period of consolidation or indecision"
        
        full_reasoning = " ".join(reasoning_parts) + f" Based on the combined analysis, the final score is {final_score:.2f}, leading to a {prediction} prediction."

        # --- 6. Generate Visualization ---
        image_path_str = None
        try:
            image_io = ImageIO()
            asset_name = trading_symbol.split('/')[0]
            
            prompt = (
                f"Epic digital painting of the cryptocurrency '{asset_name}' market forecast and market chart with real time data and indicators like ema nd rsi. "
                f"The scene depicts {trend_desc}, visualized through abstract data streams and holographic charts. "
                f"The dominant colors are {color_theme}, reflecting the {prediction.lower()} outlook. "
                f"Style: futuristic AI art, cyberpunk, high-tech, cinematic lighting, ultra-detailed."
            )
            
            safe_asset_name = re.sub(r'[^a-zA-Z0-9_-]', '_', trading_symbol)
            output_filename = f"{safe_asset_name}_{prediction}.png"
            output_dir = Path(PROJECT_ROOT) / "generated_media" / f"user_{self.user_id}" / "predictions"
            output_dir.mkdir(parents=True, exist_ok=True)
            image_output_path = output_dir / output_filename

            logger.info(f"Generating prediction image for user {self.user_id} at {image_output_path}...")
            generated_path = image_io.generate_image(prompt, image_output_path)
            
            if generated_path:
                image_path_str = str(generated_path.relative_to(Path(PROJECT_ROOT)))
                logger.info(f"Image generation successful. Path: {image_path_str}")
            else:
                logger.warning(f"Image generation failed for {trading_symbol}.")
        
        except (ValueError, ImportError) as e:
             logger.error(f"ImageIO could not be initialized, skipping image generation. Error: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during image generation: {e}", exc_info=True)

        # --- 7. Structure and Return Result ---
        return {
            "asset": trading_symbol,
            "prediction": prediction,
            "reasoning": full_reasoning,
            "image_path": image_path_str
        }

    # --- START OF MODIFIED SECTION ---
    def _prepare_llm_messages(self, system_prompt: Optional[str] = None, limit_history: Optional[int] = 15) -> List[Dict[str, str]]:
        messages: List[Dict[str, str]] = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        history_to_use = self.conversation_history[-limit_history:] if limit_history else self.conversation_history

        # Refactored to handle the new history format: [{'role': ..., 'content': ...}]
        for entry in history_to_use:
            role = entry.get("role")
            content = entry.get("content")
            if role in ["user", "assistant"] and content and isinstance(content, str):
                messages.append({"role": role, "content": content})

        return messages
    # --- END OF MODIFIED SECTION ---

    def _call_llm_api_sync(self, messages: List[Dict[str, str]], intent_detection: bool = False) -> Optional[str]:
        if not self.llm_available:
            logger.warning("LLM API call skipped: LLM client is not available (API key missing or config error).")
            return "My language generation abilities are currently limited as I cannot connect to the LLM service."

        max_tokens_to_use = 60 if intent_detection else self.llm_config.max_tokens
        temp_to_use = 0.1 if intent_detection else self.llm_config.temperature

        headers = {
            "Authorization": f"Bearer {self.llm_config.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "http://localhost",
            "X-Title": "Voxaroid Assistant",
        }
        payload = {
            "model": self.llm_config.model,
            "messages": messages,
            "max_tokens": max_tokens_to_use,
            "temperature": temp_to_use,
            "top_p": self.llm_config.top_p,
        }

        try:
            logger.debug(f"Calling OpenRouter API (synchronously). Model: {self.llm_config.model}, MaxTokens: {max_tokens_to_use}, Temp: {temp_to_use}, Messages Count: {len(messages)}")

            response = requests.post(
                OPENROUTER_API_URL,
                headers=headers,
                json=payload,
                timeout=45
            )
            response.raise_for_status()

            result = response.json()
            logger.debug(f"OpenRouter API Full Response (Status {response.status_code}): {str(result)[:300]}...")

            if result.get("choices") and result["choices"][0].get("message"):
                generated_text = result["choices"][0]["message"]["content"].strip()
                generated_text = re.sub(r'<\|.*?\|>', '', generated_text)
                logger.info(f"LLM response successfully received. Length: {len(generated_text)} chars.")
                return generated_text
            else:
                logger.warning(f"Unexpected OpenRouter API response format. No choices or message found: {result}")
                return "Sorry, I received an unexpected response format from the language model."

        except requests.exceptions.Timeout:
            logger.error("OpenRouter API request timed out after 45 seconds.")
            return "Sorry, the request to the language model timed out. The model might be busy or taking too long. Please try again in a moment."
        except requests.exceptions.HTTPError as http_err:
            status_code = http_err.response.status_code
            error_detail = http_err.response.text[:200] if http_err.response.text else str(http_err)
            logger.error(f"HTTP error calling OpenRouter API (Status {status_code}): {error_detail}", exc_info=True)
            if status_code == 401: return "Sorry, there's an authentication issue with the LLM API key. Please verify your credentials."
            if status_code == 402: return "Sorry, it seems your OpenRouter account has insufficient funds or credits for this request."
            if status_code == 429: return "Sorry, the LLM API rate limit has been reached. Please wait a moment before trying again."
            if status_code == 503: return "Sorry, the language model is temporarily unavailable or currently loading. Please try again shortly."
            return f"Sorry, I encountered an API error (Status: {status_code}). Details: {error_detail}"
        except requests.exceptions.RequestException as e:
            logger.error(f"Network or Request error calling OpenRouter API: {e}", exc_info=True)
            return f"Sorry, I encountered a network error while trying to communicate with the LLM. Please check your internet connection. Details: {str(e)[:100]}"
        except Exception as e:
            logger.error(f"Unexpected error during synchronous LLM API call: {e}", exc_info=True)
            return "Sorry, an unexpected error occurred while I was trying to process your request with the language model."

    async def _call_llm_api(self, messages: List[Dict[str, str]], intent_detection: bool = False) -> Optional[str]:
        if not self.llm_available:
             return "My language generation abilities are currently limited because I can't connect to the advanced LLM service. However, I can still assist with specific data requests and keyword-based analyses if you need them!"
        try:
            response_text = await asyncio.to_thread(
                self._call_llm_api_sync,
                messages,
                intent_detection=intent_detection
            )
            return response_text
        except Exception as e:
            logger.error(f"Error running _call_llm_api_sync in thread: {e}", exc_info=True)
            return "Sorry, there was an issue dispatching the request to the language model."

    async def _extract_coin_symbol(self, query: str) -> Optional[str]:
        """
        MODIFIED: Uses an LLM to dynamically extract the cryptocurrency symbol from the user's query.
        The extracted symbol is then validated against the cached list of known symbols.
        """
        if not self.llm_available:
            logger.warning("LLM-based symbol extraction skipped: LLM is not available. This feature will not work.")
            return None

        try:
            # Construct a highly specific prompt for the LLM to act as an entity extractor.
            extraction_prompt = (
                "You are an expert entity extraction system. Your only task is to identify a single cryptocurrency from the user's text and return its official trading symbol.\n\n"
                f"User text: \"{query}\"\n\n"
                "Instructions:\n"
                "1. Find the cryptocurrency mentioned (e.g., 'bitcoin', 'ETH', 'solana', 'doge').\n"
                "2. Return ONLY its standard, uppercase trading symbol (e.g., BTC, ETH, SOL, DOGE).\n"
                "3. If no cryptocurrency is found or you are uncertain, return the word 'NONE'.\n"
                "4. Do not include any explanations or conversational text. Your entire response must be just the symbol or 'NONE'."
            )

            messages = [{"role": "user", "content": extraction_prompt}]

            # Use the specialized LLM call settings (low temp, low max_tokens)
            logger.info(f"Attempting to extract symbol from '{query}' using LLM...")
            llm_response = await self._call_llm_api(messages, intent_detection=True)

            if not llm_response or llm_response.strip().upper() in ["NONE", ""]:
                logger.info("LLM indicated no symbol found in the query.")
                return None

            # Clean and validate the potential symbol from the LLM
            potential_symbol = llm_response.strip().upper()
            potential_symbol = re.sub(r'[^A-Z0-9]', '', potential_symbol) # Remove any stray punctuation

            # Sanity check: is it a plausible symbol format?
            if not (2 <= len(potential_symbol) <= 10):
                logger.warning(f"LLM returned an implausible symbol format: '{llm_response}'. Discarding.")
                return None

            # CRITICAL VERIFICATION STEP: Check if the LLM's extracted symbol exists in our cached list.
            if self.known_symbols and potential_symbol in self.known_symbols:
                logger.info(f"LLM extracted symbol '{potential_symbol}' and it was successfully validated against the cached exchange list.")
                self.context["last_symbol_extracted"] = potential_symbol
                return potential_symbol
            elif not self.known_symbols:
                logger.warning("Symbol cache is empty. Tentatively accepting LLM-extracted symbol '{potential_symbol}' without verification.")
                self.context["last_symbol_extracted"] = potential_symbol
                return potential_symbol
            else:
                logger.warning(f"LLM extracted symbol '{potential_symbol}', but it was NOT found in the cached list of {len(self.known_symbols)} known symbols. Discarding as a likely hallucination.")
                return None

        except Exception as e:
            logger.error(f"An unexpected error occurred during LLM-based symbol extraction: {e}", exc_info=True)
            return None

    def _get_symbol_for_module(self, extracted_entity: Optional[str]) -> Optional[str]:
        target_identifier = extracted_entity or self.context.get("last_symbol_extracted") or self.module_config.default_symbol

        if not target_identifier:
            logger.error("Cannot resolve symbol: No identifier provided, in context, or as default.")
            return None

        logger.info(f"Resolving '{target_identifier}' to a trading pair using CoinSymbolMapper...")
        trading_pair = get_symbol(target_identifier)

        if trading_pair:
            logger.info(f"Successfully mapped '{target_identifier}' to '{trading_pair}'.")
            return trading_pair.upper()
        else:
            logger.warning(f"Could not resolve '{target_identifier}' to a trading pair via mapper. This may cause issues with data fetching.")
            if '/' in target_identifier:
                return target_identifier.upper()
            return f"{target_identifier.upper()}/USDT"

    async def _detect_intent(self, query: str) -> str:
        query_lower = query.lower()
        matched_intent = "unknown"

        actionable_priority_intents = [
            "predict_price", "generate_signal", "explain_signal", "fetch_market_data", "analyze_indicators",
            "analyze_patterns", "analyze_sentiment", "analyze_macro", "analyze_anomalies", "analyze_visual_document",
            "fetch_macro_news", "farewell"
        ]

        detected_keyword_intents = []

        for intent_key, keywords_list in INTENT_KEYWORDS.items():
            if any(re.search(r'\b' + re.escape(keyword) + r'\b', query_lower) for keyword in keywords_list):
                detected_keyword_intents.append(intent_key)

        if detected_keyword_intents:
            priority_match_found = next((i for i in actionable_priority_intents if i in detected_keyword_intents), None)
            if priority_match_found:
                matched_intent = priority_match_found
            else:
                matched_intent = detected_keyword_intents[0]
            logger.info(f"Intent detected via keywords: '{matched_intent}' (from possible: {detected_keyword_intents})")

        if matched_intent == "unknown" and self.intent_config.use_llm_for_intent and self.llm_available:
            logger.info(f"No definitive keyword match for intent. Attempting LLM-based intent classification for query: '{query}'")

            # --- START OF MODIFIED SECTION ---
            # Refactored to handle the new history format
            history_for_prompt_list = []
            recent_history_entries = self.conversation_history[-(self.intent_config.max_history_for_intent):]
            for entry in recent_history_entries:
                role = entry.get("role", "unknown").capitalize()
                content = entry.get("content")
                if content:
                    history_for_prompt_list.append(f"{role}: {content}")
            history_str_for_llm = "\n".join(history_for_prompt_list) if history_for_prompt_list else "None"
            # --- END OF MODIFIED SECTION ---

            classification_prompt_str = self.intent_config.prompt_template.format(
                intents=", ".join(self.intent_config.intents),
                history=history_str_for_llm,
                query=query
            )
            messages_for_llm_intent = [{"role": "user", "content": classification_prompt_str}]

            llm_intent_response = await self._call_llm_api(messages_for_llm_intent, intent_detection=True)

            if llm_intent_response and isinstance(llm_intent_response, str):
                potential_intent_from_llm = llm_intent_response.split('\n')[0].strip().lower().replace("intent:", "").strip()

                validated_llm_intent = "unknown"
                for known_intent_val in self.intent_config.intents:
                    k_lower = known_intent_val.lower()
                    p_lower = potential_intent_from_llm
                    if k_lower == p_lower or \
                       k_lower.replace("_", "") == p_lower.replace("_", "") or \
                       k_lower.replace("_", " ") == p_lower.replace("_", " ") or \
                       ("_" in k_lower and k_lower.split('_')[-1] == p_lower):
                        validated_llm_intent = known_intent_val
                        break

                if validated_llm_intent != "unknown":
                    matched_intent = validated_llm_intent
                    logger.info(f"Intent detected via LLM: '{matched_intent}' (Raw LLM: '{llm_intent_response}')")
                else:
                    logger.warning(f"LLM returned an unrecognized or unhandled intent: '{llm_intent_response}'. Defaulting to 'unknown'.")
            else:
                logger.error(f"LLM intent classification call failed or returned invalid data: '{llm_intent_response}'. Using 'unknown' intent.")
        elif matched_intent == "unknown" and self.intent_config.use_llm_for_intent and not self.llm_available:
             logger.warning("Keyword match failed for intent. LLM intent classification is enabled but LLM is currently unavailable. Defaulting to 'unknown'.")
        elif matched_intent == "unknown":
             logger.info("No keyword match for intent, and LLM intent classification is disabled or did not yield a result. Intent remains 'unknown'.")

        self.context["last_intent"] = matched_intent
        return matched_intent

    def _format_error_message(self, module_name: str, error: Any) -> str:
        error_type = type(error).__name__
        logger.error(f"Error encountered during {module_name}: {error_type} - {str(error)}", exc_info=True)
        user_message = (f"Ndo (I'm sorry)! I encountered an issue while trying to '{module_name}'. "
                        f"It seems there was a '{error_type}' problem. Please try again, or perhaps ask in a different way. "
                        f"If this keeps happening, my human friends might need to check the system logs.")
        return user_message

    def _print_typing_effect(self, text: str):
        if not isinstance(text, str):
            logger.warning(f"Typing effect received non-string input of type: {type(text)}. Converting to string.")
            text = str(text)

        if self.output_config.verbose and self.output_config.typing_delay > 0:
            for char_to_print in text:
                print(char_to_print, end='', flush=True)
                time.sleep(self.output_config.typing_delay)
            print()
        else:
            print(text)

    async def _call_llm_for_conversation(self, query: str, task_prompt: Optional[str] = None) -> str:
        if not self.llm_available:
            if "joke" in query.lower(): return "I'd tell you a crypto joke, but I'm afraid it wouldn't make any cents right now without my LLM brain!"
            if "how are you" in query.lower(): return "I'm operational and ready to process data! My advanced conversational circuits are offline, though."
            return ("My advanced conversational abilities are offline due to an LLM connection issue. "
                    "However, I can still help with specific commands like 'signal for BTC', 'analyze patterns for ETH', etc. "
                    "What can I do for you?")

        current_dt = datetime.now()
        current_date_str = current_dt.strftime('%A, %B %d, %Y')
        current_time_str = current_dt.strftime('%I:%M %p %Z')
        location = "Nigeria"

        base_system_prompt = (
            f"You are 'Voxaroid Assistant', a highly knowledgeable, friendly, and witty AI specializing in cryptocurrency trading analysis, signals and can be a great conversationist even outside crytocurrency and trading. "
            f"You are proudly based in {location}. Your personality is helpful, encouraging, articulate, and professional, with a touch of appropriate Nigerian flair (e.g., use 'who dey breet?', 'hawfar', 'as in dey be', 'Nna men', 'Omo!', 'No wahala' sparingly and naturally if the context fits). "
            f"Your core functions are: providing trading signals, performing detailed market analysis (technical indicators, candlestick patterns, sentiment, macroeconomic factors, anomaly detection, and visual document analysis). " # INTEGRATED: Added visual analysis to capabilities
            f"You also discuss broader topics like cryptocurrency news, blockchain technology, decentralized finance (DeFi), NFTs, the global economy, and relevant geopolitical events. "
            f"Current Date: {current_date_str}. Current Time: {current_time_str}. Your Location: {location}.\n"
            f"Context from our conversation with User ID {self.user_id}: User's last recognized intent was '{self.context.get('last_intent', 'not specified')}', and the last crypto asset mentioned was '{self.context.get('last_symbol_extracted', 'not specified')}'.\n\n"
            f"{'Specific Task for this response: ' + task_prompt if task_prompt else 'Task: Engage in a natural, helpful conversation based on the user query and our ongoing chat history. Leverage your knowledge base for crypto, economic, and geopolitical topics.'}"
        )

        messages_for_llm = self._prepare_llm_messages(system_prompt=base_system_prompt, limit_history=10)

        current_query_message = {"role": "user", "content": query}
        if not messages_for_llm or messages_for_llm[-1].get("role") != "user" or messages_for_llm[-1].get("content") != query:
             if messages_for_llm and messages_for_llm[-1].get("role") == "assistant":
                 messages_for_llm.append(current_query_message)
             elif messages_for_llm and messages_for_llm[-1].get("role") == "user":
                  messages_for_llm[-1] = current_query_message
             else:
                  messages_for_llm.append(current_query_message)

        response = await self._call_llm_api(messages_for_llm)
        return response if response else "Apologies, I couldn't formulate a response at this moment. Could you please try rephrasing or asking something different?"

    async def _handle_predict_price(self, symbol_for_module: Optional[str]) -> str:
        """Handles the 'predict_price' intent."""
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for a price prediction without specifying a valid coin.",
                task_prompt="Politely ask the user to specify which cryptocurrency (e.g., BTC/USDT, Solana) they want a price prediction for."
            )

        try:
            self._print_typing_effect("Maka, I'm analyzing the market forces... This involves a lot of data, so give me a moment...")
            
            # Run the synchronous, intensive prediction function in a separate thread
            prediction_result = await asyncio.to_thread(self.prediction_price, symbol_for_module)

            if not prediction_result:
                task_prompt = (f"Inform the user that the price prediction for {symbol_for_module} failed. "
                               f"This was likely due to an inability to fetch sufficient data or an error in one of the analysis modules. "
                               f"Suggest trying another asset or checking back later.")
                return await self._call_llm_for_conversation(
                    query=f"Internal failure during price prediction for {symbol_for_module}",
                    task_prompt=task_prompt
                )

            # Successfully got a result, now format it for the user
            asset = prediction_result.get('asset')
            prediction = prediction_result.get('prediction')
            reasoning = prediction_result.get('reasoning')
            image_path = prediction_result.get('image_path') # e.g., 'generated_media/predictions/BTC-USDT_Bullish.png'

            # Build the text response
            response_text = (
                f"🔮 **Price Prediction for {asset}**\n\n"
                f"**Outlook:** **{prediction}**\n\n"
                f"**Reasoning:** {reasoning}\n\n"
            )

            # Add the image URL if it was created
            if image_path:
                # The server is configured to serve from PROJECT_ROOT.
                # The URL needs to be accessible from the client. Assuming localhost for now.
                # The HTTP server runs on port 8001 as per Server.py
                image_url = f"https://cryptsignal-backend.onrender.com/{image_path.replace(os.path.sep, '/')}"
                response_text += f"🖼️ **Visualization:** [View Prediction Art]({image_url})"
            else:
                response_text += "*(Visualization could not be generated for this prediction.)*"
            
            response_text += "\n\n**Disclaimer:** This is an AI-generated forecast and NOT financial advice. Always conduct your own research and manage your risk."

            return response_text

        except Exception as e:
            return self._format_error_message(f"Price Prediction for {symbol_for_module}", e)

    async def _handle_generate_signal(self, symbol_for_module: Optional[str]) -> str:
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for a trading signal without specifying a valid coin or trading pair.",
                task_prompt="Politely ask the user to specify which coin or trading pair (e.g., BTC/USDT, Solana, ETH) they want a trading signal for. Explain that this information is crucial for the analysis."
            )

        try:
            logger.info(f"Attempting to generate MTF signal for trading pair: {symbol_for_module}...")
            self.signal_generator = SignalGenerator(
                default_timeframe=self.module_config.default_timeframe,
                default_limit=self.module_config.default_limit
            )

            if not hasattr(self.signal_generator, 'generate_mft_signal'):
                logger.error("SignalGenerator class is not correctly loaded or is a dummy. Cannot generate signal.")
                return self._format_error_message("Signal Generation (Module Load Error)", ImportError("SignalGenerator module not loaded or outdated"))

            signal_result: Optional[Dict] = await asyncio.to_thread(
                self.signal_generator.generate_mft_signal,
                trading_symbol=symbol_for_module
            )

            if not signal_result:
                logger.warning(f"MTF SignalGenerator returned None for {symbol_for_module}. This may indicate an internal issue.")
                task_prompt = (f"Explain to the user that an unexpected issue occurred while trying to generate a signal for {symbol_for_module}, and no information could be retrieved. "
                               f"Suggest trying again later or checking a different asset.")
                return await self._call_llm_for_conversation(query=f"Internal issue generating signal for {symbol_for_module}", task_prompt=task_prompt)

            self.last_signal = signal_result
            signal_type = signal_result.get("signal_type")

            if signal_type == "HOLD":
                hold_reason = signal_result.get('reason', "Market conditions are neutral, conflicting, or the trend is too weak.")
                task_prompt = (f"Explain to the user that no strong BUY or SELL signal could be generated for {symbol_for_module} at this time. "
                               f"A 'HOLD' or neutral stance is advised. "
                               f"The reason provided by the Multi-Timeframe analysis is: '{hold_reason}'. "
                               f"Encourage patience or checking another asset. Remind them this is not financial advice.")
                query_context_for_llm = f"Explain HOLD signal for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)

            elif signal_type in ["LONG", "SHORT"]:
                signal_time_dt = datetime.fromisoformat(signal_result.get('timestamp_utc', datetime.utcnow().isoformat()))
                signal_time_str = signal_time_dt.strftime('%Y-%m-%d %H:%M:%S %Z')

                summary = (
                    f"Action: {signal_result.get('signal_type', 'N/A')}\n"
                    f"  Entry: ${signal_result.get('entry_price', 0.0):,.5f}\n"
                    f"  Stop Loss: ${signal_result.get('stop_loss', 0.0):,.5f}\n"
                    f"  Take Profit 1: ${signal_result.get('take_profit_1', 0.0):,.5f}\n"
                    f"  Take Profit 2: ${signal_result.get('take_profit_2', 0.0):,.5f}\n"
                    f"  Confidence: {signal_result.get('confidence', 'N/A')} (Score: {signal_result.get('score', 0.0):.2f})\n"
                    f"  Suggested Leverage: {signal_result.get('suggested_leverage', 'N/A')}\n"
                    f"  Valid Until: {signal_result.get('valid_until', 'N/A')}\n"
                    f"  Analysis Summary: {signal_result.get('reason', 'Analysis based on multiple timeframes.')[:250]}..."
                )

                task_prompt = (f"Present the following generated trading signal for {symbol_for_module} (Timeframes: {signal_result.get('timeframe', 'N/A')}) clearly and professionally. "
                               f"Signal generated around {signal_time_str}. Signal Details:\n{summary}\n"
                               f"**Crucially, emphasize that this is NOT financial advice (NFA), users MUST conduct their own research (DYOR), and manage their risk effectively.** You can use a phrase like 'Remember, market no be your mate, trade wisely!'")
                query_context_for_llm = f"Present generated signal for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
            else:
                logger.error(f"SignalGenerator for {symbol_for_module} returned an unexpected signal_type: '{signal_type}'. Full result: {signal_result}")
                return await self._call_llm_for_conversation(query=f"Unexpected signal result for {symbol_for_module}", task_prompt="Inform the user that the signal generation process completed but returned an unexpected result. Suggest trying again.")

        except Exception as e:
            return self._format_error_message(f"Signal Generation for {symbol_for_module}", e)

    # --- INTEGRATED: New handler for visual analysis ---
    async def _handle_analyze_visual_document(self, query: str) -> str:
        """Handles the 'analyze_visual_document' intent."""
        if not self.visual_analyzer:
            return "I'm sorry, my visual analysis module is not available. Please check if the OpenAI API key is configured correctly."

        # Regex to find a file path (quoted or unquoted) and the prompt
        # Example: analyze file "C:/Users/test.pdf" and summarize its content
        match = re.search(r'file\s+("([^"]+)"|([^\s"]+))\s*(.*)', query, re.IGNORECASE)

        if not match:
            return await self._call_llm_for_conversation(
                query="User asked to analyze a file but didn't provide a path.",
                task_prompt="Politely ask the user to provide the full path to the file they want to analyze. Explain that they should use a format like: 'analyze file \"/path/to/your/document.pdf\" and summarize it'."
            )

        # The path is either in group 2 (quoted) or group 3 (unquoted/no spaces)
        file_path_str = match.group(2) or match.group(3)
        # The rest of the query is the prompt
        prompt = match.group(4).strip()

        if not prompt:
            prompt = "Please provide a detailed description of this file's content." # Default prompt

        file_path = Path(file_path_str).resolve()
        logger.info(f"Initiating visual analysis for file: '{file_path}' with prompt: '{prompt}'")
        self._print_typing_effect(f"Okay, let me take a look at the file at '{file_path}'. Analyzing now...")

        try:
            # The analyze method in VisualAnalyzer is async
            analysis_result = await self.visual_analyzer.analyze(file_path, prompt)

            if "Error:" in analysis_result or "Sorry, I could not" in analysis_result or "Unsupported" in analysis_result:
                 logger.warning(f"Visual analysis returned a user-facing error: {analysis_result}")
                 # Let the LLM format the error nicely
                 task_prompt = f"The visual analysis module failed to process the file '{file_path}'. The reason given was: '{analysis_result}'. Relay this information to the user in a helpful way, suggesting they check the file path, permissions, and ensure the file is not corrupted or is a supported format."
                 return await self._call_llm_for_conversation(query="Visual analysis failed", task_prompt=task_prompt)

            logger.info("Visual analysis successful.")
            # Have the LLM format the result to keep the persona consistent.
            task_prompt = (f"You have received an analysis of a user-provided file ('{file_path.name}'). The user's prompt was: '{prompt}'. "
                           f"Here is the result from the analysis module:\n\n---\n{analysis_result}\n---\n\n"
                           f"Present this result to the user in a clear, well-formatted, and conversational manner. Start by acknowledging their request.")
            return await self._call_llm_for_conversation(query=f"Present analysis of file {file_path.name}", task_prompt=task_prompt)

        except Exception as e:
            logger.error(f"An unexpected error occurred in _handle_analyze_visual_document for file {file_path}: {e}", exc_info=True)
            return self._format_error_message(f"Visual Analysis of '{file_path.name}'", e)
    # --- END INTEGRATION ---

    async def _handle_explain_signal(self) -> str:
        if not self.last_signal:
            return await self._call_llm_for_conversation(
                query="User asked to explain a signal, but no signal was generated recently.",
                task_prompt="Inform the user that you haven't generated a signal recently that can be explained. Ask if they would like you to generate one now, and if so, for which coin or trading pair."
            )
        try:
            signal_asset = self.last_signal.get('asset', 'the last analyzed asset')
            signal_action = self.last_signal.get('signal_type', 'N/A')
            signal_score = self.last_signal.get('score', 'N/A')
            signal_confidence = self.last_signal.get('confidence', 'N/A')
            detailed_reason = self.last_signal.get('reason', "The decision was based on a confluence of multiple analysis factors.")

            task_prompt = (
                f"Explain the reasoning behind the last signal generated for {signal_asset}. "
                f"The signal was: {signal_action}, with a confidence of {signal_confidence} (Score: {signal_score:.2f}).\n"
                f"The analysis summary provided by the generator is: '{detailed_reason}'.\n"
                f"Elaborate on this summary in a clear, understandable way. Break down the components (e.g., what the 4h trend means, what the 1h entry implies, and the purpose of the 15m confirmation). "
                f"Reiterate that this explanation is for informational purposes and not financial advice. Users should always do their own research."
            )
            query_context_for_llm = f"Explain the last signal generated ({signal_asset} - {signal_action})"
            return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
        except Exception as e:
            asset_name_for_error = self.last_signal.get('asset', 'the previously analyzed asset') if self.last_signal else 'the previously analyzed asset'
            return self._format_error_message(f"Explaining Signal for {asset_name_for_error}", e)

    async def _handle_fetch_market_data(self, symbol_for_module: Optional[str]) -> str:
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for market data without specifying a coin or trading pair.",
                task_prompt="Politely ask the user to specify which coin or trading pair (e.g., BTC/USDT, Ethereum) they want current market data for."
            )
        try:
            logger.info(f"Fetching market data for trading pair: {symbol_for_module}...")
            if not hasattr(MarketDataFetcher, 'fetch_ohlcv'):
                logger.error("MarketDataFetcher class is missing the synchronous 'fetch_ohlcv' method.")
                return self._format_error_message("Market Data Fetch (Module Load Error)", ImportError("MarketDataFetcher synchronous method not found"))

            # CORRECTED: Removed the invalid 'exchange_name' argument.
            self.market_fetcher = MarketDataFetcher(
                timeframe=self.module_config.default_timeframe,
                limit=self.module_config.default_limit
            )
            market_df = await asyncio.to_thread(
                self.market_fetcher.fetch_ohlcv,
                symbol_for_module
            )

            if market_df is not None and not market_df.empty:
                latest_candle = market_df.iloc[-1]
                timestamp_str = latest_candle.name.strftime('%Y-%m-%d %H:%M:%S %Z') if isinstance(latest_candle.name, pd.Timestamp) else str(latest_candle.name)

                data_summary_dict = {
                    "Asset": symbol_for_module,
                    "Exchange": self.module_config.default_exchange,
                    "Timeframe": self.module_config.default_timeframe,
                    "Latest Timestamp": timestamp_str,
                    "Open": f"${latest_candle['open']:,.5f}",
                    "High": f"${latest_candle['high']:,.5f}",
                    "Low": f"${latest_candle['low']:,.5f}",
                    "Close": f"${latest_candle['close']:,.5f}",
                    "Volume": f"{latest_candle['volume']:,.2f}"
                }
                data_summary_for_llm = json.dumps(data_summary_dict)

                task_prompt = (f"Present the latest market data (OHLCV) for {symbol_for_module} clearly to the user. "
                               f"Use this structured data: {data_summary_for_llm}. "
                               f"Format it in a readable way, perhaps as a small summary or a list of key values. Mention the exchange and timeframe.")
                query_context_for_llm = f"Present latest market data for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
            else:
                error_message_context = f"Sorry, I couldn't fetch the market data for {symbol_for_module}. The symbol might be incorrect, not available on any of my connected exchanges, or there might be a temporary network issue."
                logger.warning(f"Market data fetch returned None or an empty DataFrame for {symbol_for_module}.")
                task_prompt = f"Inform the user that fetching market data for {symbol_for_module} failed. Explain the situation using this context: {error_message_context}. Suggest they check the symbol or try again later."
                query_context_for_llm = f"Explain market data fetch failure for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
        except Exception as e:
            return self._format_error_message(f"Market Data Fetch for {symbol_for_module}", e)

    async def _handle_analyze_indicators(self, symbol_for_module: Optional[str]) -> str:
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for technical indicator analysis without specifying a coin or trading pair.",
                task_prompt="Politely ask the user which coin or trading pair they want technical indicators analyzed for."
            )
        try:
            logger.info(f"Analyzing technical indicators for trading pair: {symbol_for_module}...")
            if not hasattr(MarketDataFetcher, 'fetch_ohlcv') or not hasattr(TechnicalAnalyzer, 'generate_all_indicators'):
                logger.error("MarketDataFetcher or TechnicalAnalyzer class not loaded correctly.")
                return self._format_error_message("Indicator Analysis (Module Load Error)", ImportError("Required analysis module not loaded"))

            # CORRECTED: Removed the invalid 'exchange_name' argument.
            self.market_fetcher = MarketDataFetcher(
                timeframe=self.module_config.default_timeframe,
                limit=self.module_config.default_limit
            )
            market_df = await asyncio.to_thread(self.market_fetcher.fetch_ohlcv, symbol_for_module)

            if market_df is None or market_df.empty or len(market_df) < 50:
                 task_prompt_str = (f"Inform the user that technical indicator analysis for {symbol_for_module} could not be performed "
                                    f"because the required market data (at least 50 candles) could not be fetched or was insufficient. "
                                    f"Suggest checking the symbol, trying a longer timeframe, or trying again later.")
                 return await self._call_llm_for_conversation(query=f"Data fetch failed for {symbol_for_module} for TA", task_prompt=task_prompt_str)

            self.technical_analyzer = TechnicalAnalyzer(market_df)
            indicators_df = self.technical_analyzer.generate_all_indicators()

            if indicators_df is not None and not indicators_df.empty:
                latest_indicators_series = indicators_df.iloc[-1]
                key_indicators_to_show = {}
                indicator_name_patterns = {
                    'RSI (14)': 'RSI_14', 'MACD Line': 'MACD_12_26_9', 'MACD Signal': 'MACDs_12_26_9',
                    'MACD Histogram': 'MACDh_12_26_9', 'Bollinger Bands %B': 'BBP_20_2.0', 'Upper BB': 'BBU_20_2.0',
                    'Lower BB': 'BBL_20_2.0', 'EMA (20)': 'EMA_20', 'EMA (50)': 'EMA_50', 'ATR (14)': 'ATR_14',
                    'Stoch %K': 'STOCHk_14_3_3', 'Stoch %D': 'STOCHd_14_3_3', 'ADX (14)': 'ADX_14'
                }
                for display_name, col_pattern_or_name in indicator_name_patterns.items():
                    actual_col_name = next((c for c in indicators_df.columns if c.startswith(col_pattern_or_name)), None)
                    if actual_col_name and actual_col_name in latest_indicators_series and pd.notna(latest_indicators_series[actual_col_name]):
                        key_indicators_to_show[display_name] = f"{latest_indicators_series[actual_col_name]:.2f}"
                    else:
                        key_indicators_to_show[display_name] = "N/A"

                if not any(val != "N/A" for val in key_indicators_to_show.values()):
                     task_prompt_str = (f"Inform the user that while market data for {symbol_for_module} was fetched, "
                                        f"the technical indicators could not be calculated or extracted properly. "
                                        f"Suggest trying a different timeframe or asset.")
                     return await self._call_llm_for_conversation(query=f"Indicator calculation failed for {symbol_for_module}", task_prompt=task_prompt_str)

                indicators_summary_for_llm = json.dumps(key_indicators_to_show)
                task_prompt = (f"Summarize the latest key technical indicators for {symbol_for_module} "
                               f"(Timeframe: {self.module_config.default_timeframe}). Latest values are: {indicators_summary_for_llm}. "
                               f"Briefly interpret what some of these current values might suggest. "
                               f"Keep the interpretation concise, neutral, and educational. "
                               f"**Crucially, state this is for informational purposes only and NOT financial advice.**")
                query_context_for_llm = f"Present summary of technical analysis for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
            else:
                 task_prompt_str = (f"Inform the user that technical indicators for {symbol_for_module} could not be calculated. "
                                    f"Suggest trying again after some time or checking a different asset/timeframe.")
                 return await self._call_llm_for_conversation(query=f"Indicator calculation totally failed for {symbol_for_module}", task_prompt=task_prompt_str)
        except Exception as e:
            return self._format_error_message(f"Technical Indicator Analysis for {symbol_for_module}", e)

    async def _handle_analyze_patterns(self, symbol_for_module: Optional[str]) -> str:
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for chart pattern analysis without specifying a coin or trading pair.",
                task_prompt="Politely ask the user which coin or trading pair they want candlestick patterns analyzed for."
            )
        try:
            logger.info(f"Analyzing candlestick patterns for trading pair: {symbol_for_module}...")
            if not hasattr(MarketDataFetcher, 'fetch_ohlcv') or not callable(analyze_patterns):
                logger.error("MarketDataFetcher or analyze_patterns not loaded. Cannot analyze patterns.")
                return self._format_error_message("Pattern Analysis (Module Load Error)", ImportError("Required analysis module not loaded"))

            # CORRECTED: Removed the invalid 'exchange_name' argument.
            self.market_fetcher = MarketDataFetcher(
                timeframe=self.module_config.default_timeframe,
                limit=self.module_config.default_limit
            )
            market_df = await asyncio.to_thread(self.market_fetcher.fetch_ohlcv, symbol_for_module)

            if market_df is None or market_df.empty or len(market_df) < 20:
                 task_prompt_str = (f"Inform the user that candlestick pattern analysis for {symbol_for_module} could not be performed "
                                    f"because the required market data couldn't be fetched or was insufficient. "
                                    f"Suggest checking the symbol or trying again.")
                 return await self._call_llm_for_conversation(query=f"Data fetch failed for {symbol_for_module} for patterns", task_prompt=task_prompt_str)

            loop = asyncio.get_running_loop()
            market_df.attrs['symbol'] = symbol_for_module.split('/')[0]
            pattern_results_dict = await loop.run_in_executor(None, analyze_patterns, market_df.copy())

            if pattern_results_dict and isinstance(pattern_results_dict, dict):
                overall_pattern_sentiment = pattern_results_dict.get('sentiment', 'Neutral')
                recent_bullish_signals = pattern_results_dict.get('bullish_signals', 0)
                recent_bearish_signals = pattern_results_dict.get('bearish_signals', 0)
                patterns_on_latest_candle = pattern_results_dict.get('latest_patterns', [])

                if not patterns_on_latest_candle and overall_pattern_sentiment == 'Neutral':
                    response_text = (f"Pattern analysis for {symbol_for_module} shows a neutral sentiment. "
                                     f"No specific candlestick patterns were identified on the latest candle(s).")
                elif not patterns_on_latest_candle:
                     response_text = (f"Recent pattern analysis for {symbol_for_module} suggests an overall sentiment of '{overall_pattern_sentiment}'. "
                                     f"However, no specific candlestick patterns were found on the most recent candle itself.")
                else:
                    patterns_found_str = ", ".join(patterns_on_latest_candle)
                    response_text = (f"Candlestick pattern analysis for {symbol_for_module} indicates: "
                                     f"Overall recent sentiment: '{overall_pattern_sentiment}'. "
                                     f"Pattern(s) identified on the latest candle(s): **{patterns_found_str}**.")

                final_explanation = (f"{response_text} Remember, candlestick patterns are just one piece of the puzzle. "
                                     f"This information is for educational purposes and is not financial advice.")

                task_prompt = f"Present the following pattern analysis summary for {symbol_for_module}: \"{final_explanation}\""
                query_context_for_llm = f"Present summary of candlestick pattern analysis for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
            else:
                 logger.warning(f"Pattern analysis module returned invalid or empty results for {symbol_for_module}. Raw result: {pattern_results_dict}")
                 task_prompt_str = (f"Inform the user that candlestick pattern analysis for {symbol_for_module} failed to produce results.")
                 return await self._call_llm_for_conversation(query=f"Pattern analysis module failure for {symbol_for_module}", task_prompt=task_prompt_str)
        except Exception as e:
            return self._format_error_message(f"Candlestick Pattern Analysis for {symbol_for_module}", e)

    async def _handle_analyze_sentiment(self, coin_name_for_sentiment: Optional[str]) -> str:
        target_coin_name = coin_name_for_sentiment
        if not target_coin_name:
            context_extracted_entity = self.context.get("last_symbol_extracted")
            if context_extracted_entity:
                logger.info(f"No coin name in current query for sentiment. Using context: '{context_extracted_entity}'")
                target_coin_name = context_extracted_entity
            else:
                logger.info(f"No coin name in query or context for sentiment. Using default: '{self.module_config.default_symbol}'")
                target_coin_name = self.module_config.default_symbol

        if not target_coin_name:
             return await self._call_llm_for_conversation(
                query="User asked for sentiment analysis without a clear coin name.",
                task_prompt="Politely ask the user to specify which coin *name* (e.g., Bitcoin, Ethereum, Solana) they want sentiment analysis for, as this is needed for social media and news scraping."
            )

        try:
            logger.info(f"Getting sentiment snapshot for coin name: {target_coin_name}...")
            if not asyncio.iscoroutinefunction(get_sentiment_snapshot):
                logger.error("get_sentiment_snapshot is not an async function or not correctly loaded.")
                return self._format_error_message("Sentiment Analysis (Module Load Error)", TypeError("Sentiment analysis function not awaitable"))

            sentiment_data_dict = await get_sentiment_snapshot(target_coin_name.lower())

            if sentiment_data_dict and isinstance(sentiment_data_dict, dict) and not sentiment_data_dict.get('error'):
                 avg_sentiment = sentiment_data_dict.get('average_social_sentiment_score', 'N/A')
                 if isinstance(avg_sentiment, (float, int)): avg_sentiment = f"{avg_sentiment:.2f}"

                 fg_index_data = sentiment_data_dict.get('fear_greed_index', {})
                 fg_value = fg_index_data.get('value', 'N/A')
                 fg_classification = fg_index_data.get('value_classification', 'N/A')

                 dxy_score = sentiment_data_dict.get('dxy_strength_score', 'N/A')
                 if isinstance(dxy_score, (float, int)): dxy_score = f"{dxy_score:.2f}"

                 macro_news_sentiment_list = sentiment_data_dict.get('macro_economic_news', [])
                 avg_macro_senti = "N/A"
                 if macro_news_sentiment_list and isinstance(macro_news_sentiment_list, list):
                     valid_sentis = [s.get('sentiment') for s in macro_news_sentiment_list if isinstance(s.get('sentiment'), (float, int))]
                     if valid_sentis: avg_macro_senti = f"{sum(valid_sentis)/len(valid_sentis):.2f}"


                 sentiment_summary_for_llm = (
                     f"Social Sentiment Score: {avg_sentiment}. "
                     f"Fear & Greed Index: {fg_value} ({fg_classification}). "
                     f"DXY Strength: {dxy_score}. "
                     f"Avg. Macro News Sentiment (related to coin if available): {avg_macro_senti}. "
                     f"Total Posts Analyzed (for social): {sentiment_data_dict.get('total_social_posts_analyzed', 'N/A')}."
                 )

                 task_prompt = (f"Provide a sentiment snapshot for {target_coin_name.capitalize()}. "
                                f"Use this summary data: {sentiment_summary_for_llm}. "
                                f"Explain what each key metric generally implies (e.g., high social sentiment, extreme fear on F&G, DXY impact). "
                                f"Keep it concise and informative. State that sentiment can be volatile.")
                 query_context_for_llm = f"Present sentiment analysis for {target_coin_name}"
                 return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)

            elif sentiment_data_dict and sentiment_data_dict.get('error'):
                 logger.warning(f"Sentiment analysis module for {target_coin_name} reported an error: {sentiment_data_dict['error']}")
                 task_prompt_str = (f"Inform the user that sentiment analysis for {target_coin_name} could not be completed successfully. "
                                    f"The specific issue reported was: '{sentiment_data_dict['error']}'. Suggest trying again later.")
                 return await self._call_llm_for_conversation(query=f"Sentiment analysis module error for {target_coin_name}", task_prompt=task_prompt_str)
            else:
                 logger.warning(f"Sentiment analysis returned no valid results or unexpected format for {target_coin_name}. Raw result: {sentiment_data_dict}")
                 task_prompt_str = (f"Inform the user that sentiment analysis for {target_coin_name} failed to return valid results at this time. "
                                    f"This could be due to unavailability of data from sources or an internal processing issue.")
                 return await self._call_llm_for_conversation(query=f"Sentiment analysis data unavailable for {target_coin_name}", task_prompt=task_prompt_str)
        except Exception as e:
            return self._format_error_message(f"Sentiment Analysis for {target_coin_name}", e)

    async def _handle_analyze_macro(self, query: str) -> str:
        extracted_coin_entity = self.context.get("last_symbol_extracted")

        coin_name_for_macro_module = "bitcoin"
        if extracted_coin_entity:
            coin_name_for_macro_module = extracted_coin_entity.lower()

        coin_relation_text_for_llm = f"specifically relating it to {extracted_coin_entity.capitalize()}" if extracted_coin_entity else "considering its potential impact on crypto markets generally"

        try:
            logger.info(f"Analyzing macroeconomic indicators (relevance to {coin_name_for_macro_module})...")
            if not MACRO_INDICATORS or not callable(analyze_macro_indicators):
                logger.error("MACRO_INDICATORS list or analyze_macro_indicators function not available.")
                return self._format_error_message("Macro Analysis (Module Load Error)", ImportError("Macro analysis components not loaded"))

            loop = asyncio.get_running_loop()
            indicators_to_request = ["GDP Growth Rate", "CPI Inflation", "Interest Rate", "Unemployment Rate", "ISM Manufacturing PMI"]

            macro_results_list = await loop.run_in_executor(None, analyze_macro_indicators, indicators_to_request, coin_name_for_macro_module)

            if macro_results_list and isinstance(macro_results_list, list) and not any("Error analyzing" in str(item.get("text","")) for item in macro_results_list):
                summaries_for_llm = []
                for item in macro_results_list:
                    title = item.get('title', 'N/A')
                    text_summary = item.get('text', 'No data')[:150]
                    sentiment_val = item.get('sentiment', 'N/A')
                    if isinstance(sentiment_val, (float,int)): sentiment_val = f"{sentiment_val:.2f}"
                    summaries_for_llm.append(f"{title}: {text_summary}... (Sentiment: {sentiment_val})")

                macro_results_summary_for_llm = "\n".join(summaries_for_llm)

                task_prompt = (f"Summarize the recent macroeconomic indicator analysis, {coin_relation_text_for_llm}. "
                               f"Key findings from the analysis module:\n{macro_results_summary_for_llm}\n"
                               f"Focus on the latest values/trends of these indicators and briefly interpret their potential implications for the broader economy and { 'the crypto market' if not extracted_coin_entity else extracted_coin_entity.capitalize() }. Keep it concise and balanced.")
                query_context_for_llm = f"Present macro-economic analysis (related to {extracted_coin_entity})" if extracted_coin_entity else "Present general macro-economic analysis"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)

            elif macro_results_list and any("Error analyzing" in str(item.get("text","")) for item in macro_results_list):
                 error_detail = next((item.get("text") for item in macro_results_list if "Error analyzing" in str(item.get("text",""))), "Error details not available.")
                 logger.warning(f"Macro analysis module reported an error for one or more indicators: {error_detail}")
                 task_prompt_str = (f"Inform the user that while attempting to analyze macroeconomic indicators, an error occurred for some items. "
                                    f"Specific issue: '{error_detail}'. Suggest that some macro data might be temporarily unavailable.")
                 return await self._call_llm_for_conversation(query="Macro analysis partial failure", task_prompt=task_prompt_str)
            else:
                 logger.warning(f"Macro analysis module returned no results or an unexpected format. Raw result: {macro_results_list}. Falling back to general LLM overview.")
                 task_prompt_for_fallback = (f"The specific macroeconomic indicator module failed to return data. "
                                    f"Provide a brief, general overview of the current key macroeconomic factors "
                                    f"and discuss their general impact on financial markets, including {coin_relation_text_for_llm}. Use your general knowledge up to your cutoff date.")
                 return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt_for_fallback)
        except Exception as e:
            return self._format_error_message(f"Macro-Economic Analysis (related to {extracted_coin_entity or 'general crypto'})", e)

    async def _handle_analyze_anomalies(self, symbol_for_module: Optional[str]) -> str:
        if not symbol_for_module:
            return await self._call_llm_for_conversation(
                query="User asked for market anomaly detection without specifying a coin or trading pair.",
                task_prompt="Politely ask the user which coin or trading pair they want to check for market anomalies (such as unusual price spikes or volume surges)."
            )
        try:
            logger.info(f"Detecting market anomalies for trading pair: {symbol_for_module}...")
            if 'AnomalyDetector' not in globals() or not callable(AnomalyDetector):
                logger.error("AnomalyDetector class is not correctly loaded. Cannot detect anomalies.")
                return self._format_error_message("Anomaly Detection (Module Load Error)", ImportError("AnomalyDetector module not loaded"))

            self.anomaly_detector = AnomalyDetector(
                symbol=symbol_for_module,
                exchange=self.module_config.default_exchange,
                timeframe=self.module_config.default_timeframe
            )
            anomaly_report_dict = await self.anomaly_detector.detect()

            if anomaly_report_dict and isinstance(anomaly_report_dict, dict):
                if anomaly_report_dict.get("error") and "Critical pipeline error" in anomaly_report_dict["error"]:
                    logger.error(f"Anomaly detection for {symbol_for_module} encountered a critical pipeline error: {anomaly_report_dict['error']}")
                    task_prompt_str = (f"Inform the user that the anomaly detection process for {symbol_for_module} ran into a critical internal error "
                                       f"and could not complete. Details provided by the module: '{anomaly_report_dict['error']}'.")
                    return await self._call_llm_for_conversation(query=f"Anomaly detection critical error for {symbol_for_module}", task_prompt=task_prompt_str)

                is_anomaly_detected = anomaly_report_dict.get('anomaly_detected', False)
                anomaly_score_val = anomaly_report_dict.get('anomaly_score', 'N/A')
                if isinstance(anomaly_score_val, (float, int)): anomaly_score_val = f"{anomaly_score_val:.2f}"

                contributing_factors_dict = anomaly_report_dict.get('contributing_factors', {})
                factors_summary_list = []
                if isinstance(contributing_factors_dict, dict):
                    for factor_key, factor_details in contributing_factors_dict.items():
                        if isinstance(factor_details, dict):
                            factor_desc_list = factor_details.get('factors', [])
                            if factor_desc_list and isinstance(factor_desc_list, list):
                                factors_summary_list.append(f"{factor_key.replace('_', ' ').title()}: {'. '.join(factor_desc_list)}")

                factors_summary_str = "; ".join(factors_summary_list) if factors_summary_list else "Details not specified."

                task_prompt = (f"Present the market anomaly detection report for {self.anomaly_detector.trading_symbol} "
                               f"(Timeframe analyzed: {self.module_config.default_timeframe}).\n"
                               f"Anomaly Detected: {'YES, potential unusual activity found.' if is_anomaly_detected else 'NO, market activity appears within normal parameters.'}\n"
                               f"Anomaly Score: {anomaly_score_val} (Higher scores may indicate stronger anomalies).\n"
                               f"Key Contributing Factors (if any): {factors_summary_str}\n"
                               f"Explain concisely what this means. If an anomaly is detected, advise the user to be cautious. If no anomaly, state that current conditions appear normal. This is not financial advice.")
                query_context_for_llm = f"Present market anomaly report for {symbol_for_module}"
                return await self._call_llm_for_conversation(query=query_context_for_llm, task_prompt=task_prompt)
            else:
                 logger.warning(f"Anomaly detection failed to produce a valid report for {symbol_for_module}. Raw result: {anomaly_report_dict}")
                 task_prompt_str = (f"Inform the user that the market anomaly detection process for {symbol_for_module} "
                                    f"did not produce a valid report.")
                 return await self._call_llm_for_conversation(query=f"Anomaly detection module failure for {symbol_for_module}", task_prompt=task_prompt_str)
        except Exception as e:
            return self._format_error_message(f"Anomaly Detection for {symbol_for_module}", e)

    async def _handle_fetch_macro_news(self, query: str) -> str:
        news_focus = "geopolitical events" if "geopolitical" in query.lower() else "general macroeconomic developments"
        logger.info(f"Fetching summary of {news_focus} using LLM's knowledge base.")

        task_prompt = (f"Provide a brief summary (2-3 key bullet points) of the *very latest* significant {news_focus} "
                       f"that could be relevant to global financial and cryptocurrency markets. "
                       f"Acknowledge your knowledge cutoff date if relevant when discussing 'latest' news.")
        return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt)

    async def _handle_world_economy(self, query: str) -> str:
        logger.info("Handling 'world_economy' query dynamically using LLM.")
        task_prompt = "The user is asking about the world economy or geopolitics. Provide an insightful and balanced overview based on their specific query. Discuss current trends, potential impacts on markets (including crypto if relevant), and major influencing factors. Use your general knowledge."
        return await self._call_llm_for_conversation(query, task_prompt=task_prompt)

    async def _handle_crypto_general(self, query: str) -> str:
        logger.info("Handling 'crypto_general' query dynamically using LLM.")
        task_prompt = "The user has a general question about cryptocurrency, blockchain, DeFi, NFTs, or related topics. Answer their query accurately and informatively using your knowledge base. If they ask for an opinion, you can offer a balanced perspective."
        return await self._call_llm_for_conversation(query, task_prompt=task_prompt)

    async def _handle_personal_conversation(self, query: str) -> str:
        logger.info("Handling 'personal_conversation' query dynamically using LLM.")
        task_prompt = "The user is engaging in personal conversation (e.g., asking how you are, about your 'day'). Respond in your AI assistant persona: friendly, helpful, and acknowledge you are an AI without personal feelings but always ready to assist."
        return await self._call_llm_for_conversation(query, task_prompt=task_prompt)

    async def _handle_casual_chat(self, query: str) -> str:
        logger.info("Handling 'casual_chat' (greetings, thanks) dynamically using LLM.")
        task_prompt = "The user is making casual conversation (e.g., greetings, thanks). Respond politely and appropriately in your AI assistant persona. Keep it brief and friendly."
        return await self._call_llm_for_conversation(query, task_prompt=task_prompt)

    async def _handle_unknown(self, query: str) -> str:
        logger.warning(f"Handling 'unknown' intent for query: '{query}'. Attempting dynamic LLM response.")
        task_prompt = ("The user's specific intent is unclear or not recognized by keyword matching. "
                       "Analyze their query: '{query}'. Respond helpfully based on your general knowledge domains (cryptocurrency, trading, global economy, etc.). "
                       "If the query seems completely unrelated to your functions or too ambiguous, politely ask for clarification or suggest topics you can help with.")
        return await self._call_llm_for_conversation(query, task_prompt=task_prompt)

    async def _handle_tell_joke(self, query: str) -> str:
        logger.info("Handling request to 'tell_joke'.")
        task_prompt = ("Tell a short, light-hearted, and witty joke. "
                       "If possible and appropriate, try to relate it to topics like cryptocurrency, trading, finance, technology, or AI. "
                       "Ensure the joke is generally inoffensive.")
        return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt)

    async def _handle_start_conversation(self, query: str) -> str:
        logger.info("Handling request to 'start_conversation'.")
        task_prompt = ("The user wants to start a new conversation or is inviting you to talk. "
                       "Initiate a friendly and engaging chat. You could ask an open-ended question about their current interests in the crypto space, "
                       "you can also ask them questions in other to spice up the conversation."
                       "their thoughts on recent market movements, or offer to discuss a trending topic in crypto or the global economy.")
        return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt)

    async def _handle_compliment(self, query: str) -> str:
        logger.info("Handling a 'compliment_received'.")
        task_prompt = ("The user has given you a compliment (e.g., 'good job', 'you're helpful'). "
                       "Respond graciously and modestly. Thank the user for their kind words. "
                       "You might subtly reiterate your purpose, for example, 'Thank you! I'm glad I could assist. Is there anything else I can help you with today?'")
        return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt)

    async def _handle_farewell(self, query: str) -> str:
        logger.info("Handling 'farewell' request.")
        task_prompt = ("The user intends to end the conversation (e.g., said 'bye', 'exit', 'quit'). "
                       "Respond with a polite, friendly, and conclusive closing message. Wish them well. "
                       "You could use a culturally relevant closing remark like 'Ka ọ dị!' or 'Take care!'.")
        return await self._call_llm_for_conversation(query=query, task_prompt=task_prompt)

    async def handle_query(self, query: str) -> Tuple[str, bool]:
        query = query.strip()
        if not query:
            self._print_typing_effect("Pardon? I didn't quite catch that. Could you please type your question?")
            return "Pardon? I didn't quite catch that. Could you please type your question?", False

        # --- START OF MODIFIED SECTION ---
        # History is now added to the DB transactionally. The in-memory list is updated for immediate context.
        if query != "<SYSTEM_STARTUP>":
            # Add user message to in-memory history for immediate use by the LLM
            self.conversation_history.append({"role": "user", "content": query})
            # Asynchronously save to DB
            await self._add_message_to_db(role="user", content=query)
        # --- END OF MODIFIED SECTION ---

        should_exit_conversation = False
        assistant_response_text = "I'm sorry, an unexpected issue occurred while processing your request."

        try:
            # MODIFIED: Await the new asynchronous symbol extraction method.
            extracted_coin_entity = await self._extract_coin_symbol(query)

            detected_intent = await self._detect_intent(query)
            logger.info(f"Processing Query: '{query}', Extracted Coin Entity: '{self.context.get('last_symbol_extracted')}', Detected Intent: '{detected_intent}' for User ID: {self.user_id}")

            trading_pair_for_modules = None
            coin_name_for_sentiment_module = None

            intents_needing_trading_pair = ["predict_price", "generate_signal", "fetch_market_data", "analyze_indicators", "analyze_patterns", "analyze_anomalies"]
            intents_needing_coin_name = ["analyze_sentiment"]

            if detected_intent in intents_needing_trading_pair:
                trading_pair_for_modules = self._get_symbol_for_module(extracted_coin_entity)
            elif detected_intent in intents_needing_coin_name:
                coin_name_for_sentiment_module = extracted_coin_entity or self.context.get("last_symbol_extracted") or self.module_config.default_symbol

            intent_handler_map: Dict[str, Callable] = {
                "predict_price": self._handle_predict_price,
                "generate_signal": self._handle_generate_signal,
                "fetch_market_data": self._handle_fetch_market_data,
                "analyze_indicators": self._handle_analyze_indicators,
                "analyze_patterns": self._handle_analyze_patterns,
                "analyze_anomalies": self._handle_analyze_anomalies,
                "analyze_sentiment": self._handle_analyze_sentiment,
                "analyze_macro": self._handle_analyze_macro,
                "analyze_visual_document": self._handle_analyze_visual_document, # INTEGRATED: Added new handler to map
                "fetch_macro_news": self._handle_fetch_macro_news,
                "explain_signal": self._handle_explain_signal,
                "tell_joke": self._handle_tell_joke,
                "start_conversation": self._handle_start_conversation,
                "compliment_received": self._handle_compliment,
                "farewell": self._handle_farewell,
                "world_economy": self._handle_world_economy,
                "crypto_general": self._handle_crypto_general,
                "personal_conversation": self._handle_personal_conversation,
                "casual_chat": self._handle_casual_chat,
                "unknown": self._handle_unknown
            }

            handler_to_call = intent_handler_map.get(detected_intent)

            if handler_to_call:
                if detected_intent in intents_needing_trading_pair:
                    assistant_response_text = await handler_to_call(trading_pair_for_modules)
                elif detected_intent in intents_needing_coin_name:
                    assistant_response_text = await handler_to_call(coin_name_for_sentiment_module)
                elif detected_intent == "explain_signal":
                    assistant_response_text = await handler_to_call()
                else:
                    assistant_response_text = await handler_to_call(query)

                if detected_intent == "farewell":
                    should_exit_conversation = True
            else:
                 logger.error(f"Intent '{detected_intent}' was detected but no corresponding handler function was found in the map, and it was not 'unknown'. This indicates a configuration error.")
                 assistant_response_text = await self._handle_unknown(query)

        except Exception as e:
            logger.error(f"An unexpected error occurred in handle_query for intent '{self.context.get('last_intent', 'unknown')}' with query '{query}': {e}", exc_info=True)
            assistant_response_text = self._format_error_message(f"processing your request (intent: {self.context.get('last_intent', 'unknown')})", e)
            should_exit_conversation = False
        
        # --- START OF MODIFIED SECTION ---
        # Save the assistant's response to in-memory history and the database
        if query != "<SYSTEM_STARTUP>":
            self.conversation_history.append({"role": "assistant", "content": assistant_response_text})
            await self._add_message_to_db(role="assistant", content=assistant_response_text)
        # --- END OF MODIFIED SECTION ---

        self._print_typing_effect(assistant_response_text)
        return assistant_response_text, should_exit_conversation

    async def get_dynamic_welcome(self) -> str:
        logger.info("Attempting to generate a dynamic welcome message using LLM...")
        task_prompt_for_welcome = (
            "Generate a friendly, concise, and inviting opening message for a user starting a new conversation. "
            "Introduce yourself as 'Cryptsignal Assistant', an AI based in Onitsha, Nigeria, powered by an advanced LLM. "
            "Briefly highlight your main capabilities: generating trading signals, performing comprehensive market analysis (technical, candlestick patterns, sentiment, macroeconomic factors, anomaly detection, and analyzing images/documents). " # INTEGRATED: Updated capabilities
            "Mention you can also chat about cryptocurrency trends, the global economy, and related topics. "
            "Keep the message to 2-3 sentences. Include a warm Nigerian greeting like 'Kedu ka ị melu?' (How are you doing?) or 'How body? No wahala?'."
        )

        welcome_message_from_llm = await self._call_llm_for_conversation(
            query="<SYSTEM_STARTUP_WELCOME>",
            task_prompt=task_prompt_for_welcome
        )

        if welcome_message_from_llm and "Sorry" not in welcome_message_from_llm and "apologies" not in welcome_message_from_llm.lower() and len(welcome_message_from_llm) > 20:
            logger.info("Dynamic welcome message generated successfully by LLM.")
            return welcome_message_from_llm
        else:
            logger.warning(f"LLM failed to generate a valid dynamic welcome message (response: '{str(welcome_message_from_llm)[:100]}...'). Using static fallback.")
            return self.output_config.initial_welcome_message

    # --- START OF MODIFIED SECTION ---
    async def close(self):
        """Cleanly disconnects from the database if connected."""
        if database and database.is_connected:
            logger.info("Closing database connection for NLP Handler.")
            await database.disconnect()

    def __del__(self):
        # Note: __del__ is not guaranteed to be called, especially for objects
        # part of a reference cycle. Explicitly calling close() is safer.
        logger.debug("TradingAssistantNLPHandler instance is being cleaned up.")
        if database and database.is_connected:
            # This is a fallback; direct async call isn't possible here.
            # The main loop should handle disconnection.
            logger.warning("Handler deleted while DB connection is still open. Relying on main application shutdown.")
    # --- END OF MODIFIED SECTION ---


async def main_interaction_loop():
    """
    Main asynchronous loop for interacting with the TradingAssistantNLPHandler.
    Handles user input, processes queries, and manages the conversation flow.
    """
    print("Initializing CryptoNaija Trading Assistant...")
    logging.getLogger("ccxt").setLevel(logging.WARNING)

    nlp_handler_instance = None
    
    # --- START OF MODIFIED SECTION ---
    # Manage database connection for standalone script execution
    if not database:
        logger.critical("Database is not configured. Please set the DATABASE_URL environment variable. Exiting.")
        return
    
    try:
        await database.connect()
        logger.info("Database connection established for main loop.")
        # --- END OF MODIFIED SECTION ---

        # Step 1: Create the handler instance (this is synchronous)
        # We'll use a default user_id for the interactive terminal mode.
        # In the server, this will be provided by the client.
        nlp_handler_instance = TradingAssistantNLPHandler(user_id=999)

        # Step 2: Asynchronously initialize the handler (fetches symbols, loads history from DB)
        await nlp_handler_instance.initialize()

        print("-" * 40)
        # Get and print the welcome message
        welcome_message_str = await nlp_handler_instance.get_dynamic_welcome()
        nlp_handler_instance._print_typing_effect(f"Assistant: {welcome_message_str}")
        print("-" * 40)

        while True:
            try:
                user_input_str = await asyncio.to_thread(input, "\nYou: ")
                user_input_str = user_input_str.strip()

                if not user_input_str:
                    continue

                _, should_terminate_session = await nlp_handler_instance.handle_query(user_input_str)

                if should_terminate_session:
                    break

            except (KeyboardInterrupt, EOFError):
                print("\nExiting assistant...")
                if nlp_handler_instance:
                     await nlp_handler_instance.handle_query("exit")
                else:
                     print("CryptoNaija Assistant signing off. Goodbye!")
                break
            except Exception as loop_err:
                logger.error(f"An error occurred in the main interaction loop: {loop_err}", exc_info=True)
                error_msg_to_user = nlp_handler_instance._format_error_message("processing your input", loop_err) if nlp_handler_instance else f"A critical system error occurred: {loop_err}"
                if nlp_handler_instance:
                    nlp_handler_instance._print_typing_effect(f"\nAssistant: {error_msg_to_user}")
                else:
                    print(f"\nAssistant: {error_msg_to_user}")

    except Exception as init_err:
        logger.critical(f"Failed to initialize or run the TradingAssistantNLPHandler: {init_err}", exc_info=True)
        print(f"\nCRITICAL ERROR: The Trading Assistant could not be started. Please check the logs for details. Error: {init_err}")
    # --- START OF MODIFIED SECTION ---
    finally:
        if nlp_handler_instance:
            await nlp_handler_instance.close() # Ensure handler's own resources are clean
        elif database and database.is_connected:
            logger.info("Disconnecting database from main loop.")
            await database.disconnect()
    # --- END OF MODIFIED SECTION ---


if __name__ == "__main__":
    try:
        if not logging.getLogger().hasHandlers():
             logging.basicConfig(
                 level=logging.INFO,
                 format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                 handlers=[logging.StreamHandler()]
             )
             logging.info("Fallback basic logger configured in __main__ of trading_assistant_nlp_handler.")

        # For direct execution, we use a default user_id.
        # The user_id would typically come from a logged-in session in a real app.
        user_id_for_terminal = 1
        asyncio.run(main_interaction_loop())


    except RuntimeError as rt_err:
        if "Cannot run the event loop while another loop is running" in str(rt_err):
            print("ERROR: An asyncio event loop is already running. The application cannot start another one.")
            logger.critical("Asyncio event loop conflict: Cannot run main_interaction_loop as a loop is already running.")
        else:
            logging.critical(f"An unhandled asyncio RuntimeError occurred during execution: {rt_err}", exc_info=True)
            print(f"An unexpected asyncio runtime error occurred: {rt_err}")
    except Exception as main_exc:
        logging.critical(f"A critical unhandled exception occurred in __main__: {main_exc}", exc_info=True)
        print(f"An unexpected critical error occurred, and the assistant has to stop: {main_exc}")
